{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# hyper-parameters for the model\n",
    "batch_size = 256\n",
    "cell_size = 256\n",
    "dropout_rate = 0.85 #pkeep\n",
    "epochs = 1\n",
    "alpha = 1e-5 #learning rate\n",
    "num_classes = 2\n",
    "sequence_length = 21\n",
    "svm_c = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Dataset\n",
    "Description of dataset and operations in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Loading the training features and labels\n",
    "train_data = np.load('./gru-svm/dataset/train/train_data.npy')\n",
    "train_labels = train_data[:,17]\n",
    "train_features = np.delete(arr=train_data, obj=[17], axis=1)\n",
    "train_features = train_features.astype(np.float32)\n",
    "\n",
    "# Loading the test features and labels\n",
    "test_data = np.load('./gru-svm/dataset/test/test_data.npy')\n",
    "test_labels = test_data[:,17]\n",
    "test_features = np.delete(arr=test_data, obj=[17], axis=1)\n",
    "test_features = test_features.astype(np.float32)\n",
    "\n",
    "train_size = train_features.shape[0]\n",
    "test_size = test_features.shape[0]\n",
    "\n",
    "# Modify data size to be a multiple of batch size\n",
    "train_features = train_features[:train_size-(train_size % batch_size)]\n",
    "train_labels = train_labels[:train_size-(train_size % batch_size)]\n",
    "\n",
    "test_features = test_features[:test_size-(test_size % batch_size)]\n",
    "test_labels = test_labels[:test_size-(test_size % batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  0.  2.  1.  5.  3.  0.  7.  2.  2.  2.  0.  0.  1.  0.  0.  0.  4.\n",
      "  6.  1.  1.]\n",
      "[ 7.  4.  7.  7.  0.  0.  1.  1.  0.  8.  0.  1.  1.  2.  0.  0.  0.  9.\n",
      "  3.  6.  1.]\n",
      "1\n",
      "0\n",
      "(1898240, 21)\n",
      "(420608, 21)\n"
     ]
    }
   ],
   "source": [
    "print(train_features[0])\n",
    "print(test_features[0])\n",
    "\n",
    "print(train_labels[0])\n",
    "print(test_labels[0])\n",
    "\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. GRU-SVM\n",
    "\n",
    "### Initialize variables and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "## Initialize variables and build model\n",
    "\n",
    "# Data variables\n",
    "x = tf.placeholder(dtype=tf.uint8, shape=[None, sequence_length], name='x')\n",
    "\n",
    "# [BATCH_SIZE, SEQUENCE_LENGTH, 10]\n",
    "x_onehot = tf.one_hot(indices=x, depth=10, on_value=1.0, off_value=0.0, name='x_onehot')\n",
    "\n",
    "# [BATCH_SIZE]\n",
    "y = tf.placeholder(dtype=tf.uint8, shape=[None], name='y')\n",
    "\n",
    "# [BATCH_SIZE, N_CLASSES]\n",
    "y_onehot = tf.one_hot(indices=y, depth=num_classes, on_value=1.0, off_value=-1.0, name='y_onehot')\n",
    "\n",
    "state = tf.placeholder(dtype=tf.float32, shape=[None, cell_size], name='initial_state')\n",
    "\n",
    "p_keep = tf.placeholder(dtype=tf.float32, name='p_keep')\n",
    "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
    "\n",
    "# GRU Layer\n",
    "cell = tf.contrib.rnn.GRUCell(cell_size)\n",
    "drop_cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=p_keep) \n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(drop_cell, x_onehot, initial_state=state, dtype=tf.float32)\n",
    "\n",
    "states = tf.identity(states, name='H')\n",
    "\n",
    "\n",
    "with tf.name_scope('final_training_ops'):\n",
    "    with tf.name_scope('weights'):\n",
    "        weight = tf.get_variable('weights', initializer=tf.random_normal([cell_size, num_classes], \n",
    "                                                                         stddev=0.01))\n",
    "#         self.variable_summaries(weight)\n",
    "    with tf.name_scope('biases'):\n",
    "        bias = tf.get_variable('biases', initializer=tf.constant(0.1, shape=[num_classes]))\n",
    "#         self.variable_summaries(bias) \n",
    "    hf = tf.transpose(outputs, [1, 0, 2])\n",
    "    last = tf.gather(hf, int(hf.get_shape()[0]) - 1)\n",
    "    with tf.name_scope('Wx_plus_b'):\n",
    "        output = tf.matmul(last, weight) + bias\n",
    "        tf.summary.histogram('pre-activations', output)\n",
    "\n",
    "# SVM Layer\n",
    "with tf.name_scope('svm'):\n",
    "    regularization_loss = 0.5 * tf.reduce_sum(tf.square(weight))\n",
    "    hinge_loss = tf.reduce_sum(tf.square(tf.maximum(tf.zeros([batch_size, num_classes]), \n",
    "                                                    1 - y_onehot * output)))\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = regularization_loss + svm_c * hinge_loss\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    predicted_class = tf.sign(output)\n",
    "    predicted_class = tf.identity(predicted_class, name='prediction')\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct = tf.equal(tf.argmax(predicted_class, 1), tf.argmax(y_onehot, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step [0] train -- loss : 259.5896301269531, accuracy : 0.1796875\n",
      "step [100] train -- loss : 254.0306396484375, accuracy : 0.2734375\n",
      "step [200] train -- loss : 232.5886993408203, accuracy : 0.16796875\n",
      "step [300] train -- loss : 230.5184326171875, accuracy : 0.80859375\n",
      "step [400] train -- loss : 262.80792236328125, accuracy : 0.5234375\n",
      "step [500] train -- loss : 256.8785705566406, accuracy : 0.5390625\n",
      "step [600] train -- loss : 255.4293212890625, accuracy : 0.5390625\n",
      "step [700] train -- loss : 242.49151611328125, accuracy : 0.58984375\n",
      "step [800] train -- loss : 249.51719665527344, accuracy : 0.55078125\n",
      "step [900] train -- loss : 246.8617706298828, accuracy : 0.52734375\n",
      "step [1000] train -- loss : 292.51287841796875, accuracy : 0.33203125\n",
      "step [1100] train -- loss : 255.4005126953125, accuracy : 0.6171875\n",
      "step [1200] train -- loss : 237.19503784179688, accuracy : 0.82421875\n",
      "step [1300] train -- loss : 240.17996215820312, accuracy : 0.6171875\n",
      "step [1400] train -- loss : 191.07730102539062, accuracy : 0.80078125\n",
      "step [1500] train -- loss : 191.39117431640625, accuracy : 0.76171875\n",
      "step [1600] train -- loss : 231.05970764160156, accuracy : 0.66015625\n",
      "step [1700] train -- loss : 241.47891235351562, accuracy : 0.55859375\n",
      "step [1800] train -- loss : 257.3641357421875, accuracy : 0.375\n",
      "step [1900] train -- loss : 216.3955078125, accuracy : 0.69921875\n",
      "step [2000] train -- loss : 239.24945068359375, accuracy : 0.61328125\n",
      "step [2100] train -- loss : 283.28070068359375, accuracy : 0.33984375\n",
      "step [2200] train -- loss : 209.81712341308594, accuracy : 0.6640625\n",
      "step [2300] train -- loss : 230.0498504638672, accuracy : 0.70703125\n",
      "step [2400] train -- loss : 146.85572814941406, accuracy : 0.87890625\n",
      "step [2500] train -- loss : 195.60714721679688, accuracy : 0.73046875\n",
      "step [2600] train -- loss : 177.68692016601562, accuracy : 0.796875\n",
      "step [2700] train -- loss : 224.18402099609375, accuracy : 0.65625\n",
      "step [2800] train -- loss : 183.33657836914062, accuracy : 0.78125\n",
      "step [2900] train -- loss : 162.11878967285156, accuracy : 0.7890625\n",
      "step [3000] train -- loss : 169.1434783935547, accuracy : 0.8359375\n",
      "step [3100] train -- loss : 117.41307830810547, accuracy : 0.8203125\n",
      "step [3200] train -- loss : 155.55938720703125, accuracy : 0.7734375\n",
      "step [3300] train -- loss : 109.37562561035156, accuracy : 0.8828125\n",
      "step [3400] train -- loss : 94.21780395507812, accuracy : 0.85546875\n",
      "step [3500] train -- loss : 123.3358383178711, accuracy : 0.8046875\n",
      "step [3600] train -- loss : 93.97954559326172, accuracy : 0.8671875\n",
      "step [3700] train -- loss : 93.14581298828125, accuracy : 0.85546875\n",
      "step [3800] train -- loss : 135.47933959960938, accuracy : 0.8203125\n",
      "step [3900] train -- loss : 121.5091781616211, accuracy : 0.81640625\n",
      "step [4000] train -- loss : 93.9997787475586, accuracy : 0.875\n",
      "step [4100] train -- loss : 146.54588317871094, accuracy : 0.796875\n",
      "step [4200] train -- loss : 205.55801391601562, accuracy : 0.6640625\n",
      "step [4300] train -- loss : 167.24061584472656, accuracy : 0.73828125\n",
      "step [4400] train -- loss : 232.18695068359375, accuracy : 0.58203125\n",
      "step [4500] train -- loss : 191.14785766601562, accuracy : 0.69921875\n",
      "step [4600] train -- loss : 69.6004867553711, accuracy : 0.9296875\n",
      "step [4700] train -- loss : 149.3956298828125, accuracy : 0.796875\n",
      "step [4800] train -- loss : 142.4685516357422, accuracy : 0.77734375\n",
      "step [4900] train -- loss : 86.06632995605469, accuracy : 0.90625\n",
      "step [5000] train -- loss : 154.97120666503906, accuracy : 0.7578125\n",
      "step [5100] train -- loss : 224.86183166503906, accuracy : 0.66015625\n",
      "step [5200] train -- loss : 201.6115264892578, accuracy : 0.7265625\n",
      "step [5300] train -- loss : 49.03377151489258, accuracy : 0.96875\n",
      "step [5400] train -- loss : 90.94676208496094, accuracy : 0.87890625\n",
      "step [5500] train -- loss : 187.08204650878906, accuracy : 0.72265625\n",
      "step [5600] train -- loss : 139.56419372558594, accuracy : 0.859375\n",
      "step [5700] train -- loss : 147.97264099121094, accuracy : 0.8046875\n",
      "step [5800] train -- loss : 191.09800720214844, accuracy : 0.6953125\n",
      "step [5900] train -- loss : 178.61314392089844, accuracy : 0.75390625\n",
      "step [6000] train -- loss : 150.8306427001953, accuracy : 0.79296875\n",
      "step [6100] train -- loss : 139.70668029785156, accuracy : 0.828125\n",
      "step [6200] train -- loss : 118.59297180175781, accuracy : 0.83984375\n",
      "step [6300] train -- loss : 83.68051147460938, accuracy : 0.890625\n",
      "step [6400] train -- loss : 150.1967010498047, accuracy : 0.8125\n",
      "step [6500] train -- loss : 105.3255844116211, accuracy : 0.87109375\n",
      "step [6600] train -- loss : 104.0308837890625, accuracy : 0.8515625\n",
      "step [6700] train -- loss : 90.19234466552734, accuracy : 0.88671875\n",
      "step [6800] train -- loss : 216.88818359375, accuracy : 0.6875\n",
      "step [6900] train -- loss : 260.51019287109375, accuracy : 0.6171875\n",
      "step [7000] train -- loss : 144.95445251464844, accuracy : 0.79296875\n",
      "step [7100] train -- loss : 88.95612335205078, accuracy : 0.8828125\n",
      "step [7200] train -- loss : 95.99201202392578, accuracy : 0.89453125\n",
      "step [7300] train -- loss : 164.6016387939453, accuracy : 0.7578125\n",
      "step [7400] train -- loss : 110.18940734863281, accuracy : 0.83203125\n",
      "Starting with the testing phase\n",
      "step [100] test -- loss : 194.06602478027344, accuracy : 0.73828125, overall accuracy: 0.6710628094059404\n",
      "step [200] test -- loss : 171.2953643798828, accuracy : 0.76953125, overall accuracy: 0.6995102611940298\n",
      "step [300] test -- loss : 185.4829864501953, accuracy : 0.76953125, overall accuracy: 0.7407599667774085\n",
      "step [400] test -- loss : 218.1788330078125, accuracy : 0.6484375, overall accuracy: 0.7327774314214461\n",
      "step [500] test -- loss : 206.895263671875, accuracy : 0.7265625, overall accuracy: 0.7309131736526944\n",
      "step [600] test -- loss : 193.20556640625, accuracy : 0.80078125, overall accuracy: 0.7301177724625624\n",
      "step [700] test -- loss : 156.763671875, accuracy : 0.83984375, overall accuracy: 0.7338121879457922\n",
      "step [800] test -- loss : 177.4271697998047, accuracy : 0.77734375, overall accuracy: 0.7450940230961303\n",
      "step [900] test -- loss : 132.4095458984375, accuracy : 0.87890625, overall accuracy: 0.7477195477247507\n",
      "step [1000] test -- loss : 188.4165496826172, accuracy : 0.71875, overall accuracy: 0.7459805819180823\n",
      "step [1100] test -- loss : 147.3248291015625, accuracy : 0.84765625, overall accuracy: 0.747860609673025\n",
      "step [1200] test -- loss : 223.0601348876953, accuracy : 0.69921875, overall accuracy: 0.7444479860532894\n",
      "step [1300] test -- loss : 152.953125, accuracy : 0.83203125, overall accuracy: 0.7430822444273641\n",
      "step [1400] test -- loss : 193.28567504882812, accuracy : 0.69140625, overall accuracy: 0.7444905424696645\n",
      "step [1500] test -- loss : 213.6309051513672, accuracy : 0.76171875, overall accuracy: 0.7415655188207861\n",
      "step [1600] test -- loss : 181.580078125, accuracy : 0.78125, overall accuracy: 0.7396793019987508\n"
     ]
    }
   ],
   "source": [
    "current_state = np.zeros([batch_size, cell_size])\n",
    "\n",
    "# variables initializer\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for step in range(epochs * train_size // batch_size):\n",
    "        offset = (step * batch_size) % train_size\n",
    "        train_data=[train_features, train_labels]\n",
    "        train_example_batch = train_data[0][offset:(offset + batch_size)]\n",
    "        train_label_batch = train_data[1][offset:(offset + batch_size)]\n",
    "\n",
    "        # dictionary for key-value pair input for training\n",
    "        feed_dict = {x: train_example_batch, y: train_label_batch,\n",
    "                                 state: current_state,\n",
    "                                 learning_rate: alpha, p_keep: dropout_rate}\n",
    "\n",
    "        train_summary, _, predictions, actual, next_state = sess.run([merged, optimizer,\n",
    "                                                                                  predicted_class, y_onehot,\n",
    "                                                                                  states],\n",
    "                                                                                 feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            # get train loss and accuracy\n",
    "            train_loss, train_accuracy = sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "\n",
    "            # display train loss and accuracy\n",
    "            print('step [{}] train -- loss : {}, accuracy : {}'.format(step, train_loss, train_accuracy))\n",
    "\n",
    "        current_state = next_state\n",
    "    \n",
    "    print('Starting with the testing phase')\n",
    "    overall_accuracy = 0  #Initialize variable\n",
    "    # Testing the model\n",
    "    for step in range(epochs * test_size // batch_size):\n",
    "\n",
    "        offset = (step * batch_size) % test_size\n",
    "        test_data=[test_features, test_labels]\n",
    "        test_example_batch = test_data[0][offset:(offset + batch_size)]\n",
    "        test_label_batch = test_data[1][offset:(offset + batch_size)]\n",
    "\n",
    "#         print(test_example_batch.shape)\n",
    "#         print(test_label_batch.shape)\n",
    "\n",
    "        # dictionary for key-value pair input for validation\n",
    "        feed_dict = {x: test_example_batch, y: test_label_batch,\n",
    "        state: np.zeros([batch_size, cell_size]), p_keep: 1.0}\n",
    "\n",
    "        test_summary, predictions, actual, test_loss, test_accuracy = \\\n",
    "                        sess.run([merged, predicted_class, y_onehot, loss, accuracy],\n",
    "                                 feed_dict=feed_dict)\n",
    "        overall_accuracy = (overall_accuracy*(step) + test_accuracy)/(step+1)\n",
    "        \n",
    "        # Display test loss and accuracy every 100 steps\n",
    "        if step % 100 == 0 and step > 0:\n",
    "\n",
    "        # add the validation summary\n",
    "        # validation_writer.add_summary(validation_summary, step)\n",
    "\n",
    "        # display test loss and accuracy\n",
    "            print('step [{}] test -- loss : {}, accuracy : {}, overall accuracy: {}'.\n",
    "                  format(step, test_loss, test_accuracy,overall_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
